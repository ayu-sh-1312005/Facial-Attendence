{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34cbfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import face_recognition\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvzone\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "912fce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import db\n",
    "from firebase_admin import storage \n",
    "# firebase_admin.delete_app(firebase_admin.get_app())\n",
    "if not firebase_admin._apps:\n",
    "    cred = credentials.Certificate(r\"C:\\Users\\ayush\\OneDrive\\Pictures\\realtimeattendence-firebase-adminsdk-cb6st-ddf1f7eda0.json\")\n",
    "    firebase_admin = firebase_admin.initialize_app(cred, {'databaseURL': 'https://realtimeattendence-default-rtdb.firebaseio.com/',\n",
    "                            'storageBucket':'realtimeattendence.appspot.com'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee9273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['111', '112', '113']\n"
     ]
    }
   ],
   "source": [
    "file=open(r\"C:\\Users\\ayush\\EncodeFilet.p\",\"rb\")\n",
    "# load the known encode data with the student id\n",
    "# pickle.load(file) returns the data in same format which has been dumped in the file without any changes\n",
    "[studentId,encodeListKnown]=pickle.load(file)\n",
    "print(studentId)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7012263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the changing screen images during the process of marking the attendence\n",
    "# screen where we will display our data\n",
    "imgModeList=[]\n",
    "# get the list of images store in the data\n",
    "modePath=r\"C:\\Users\\ayush\\OneDrive\\Pictures\\Modes\"\n",
    "modeList=os.listdir(modePath)\n",
    "# read all the data using cv2 and store it in a list imgModeList\n",
    "for i in modeList:\n",
    "    temp=cv2.imread(modePath+\"\\\\\"+i)\n",
    "    imgModeList.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de030fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known face detected\n",
      "113\n",
      "+++++ 117.22694\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "+++++ 60.268949\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "+++++ 7.456545\n",
      "Known face detected\n",
      "113\n",
      "+++++ 8.23331\n",
      "Known face detected\n",
      "113\n",
      "+++++ 9.343531\n",
      "+++++ 9.628328\n",
      "+++++ 10.373575\n",
      "+++++ 11.220385\n",
      "+++++ 12.112426\n",
      "+++++ 13.054762\n",
      "Known face detected\n",
      "112\n",
      "+++++ 222.213515\n",
      "Known face detected\n",
      "112\n",
      "Known face detected\n",
      "112\n",
      "Known face detected\n",
      "112\n",
      "Known face detected\n",
      "112\n",
      "Known face detected\n",
      "112\n",
      "Known face detected\n",
      "112\n",
      "Known face detected\n",
      "112\n",
      "Known face detected\n",
      "112\n",
      "+++++ 10.542274\n",
      "Known face detected\n",
      "112\n",
      "+++++ 11.303108\n",
      "Known face detected\n",
      "112\n",
      "+++++ 12.078204\n",
      "+++++ 12.840997\n",
      "Known face detected\n",
      "111\n",
      "+++++ 203.970507\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "113\n",
      "+++++ 71.94987\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "113\n",
      "Known face detected\n",
      "111\n",
      "+++++ 44.850009\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "Known face detected\n",
      "111\n",
      "+++++ 10.372556\n",
      "Known face detected\n",
      "111\n",
      "+++++ 11.133904\n",
      "Known face detected\n",
      "111\n",
      "+++++ 11.861047\n",
      "Known face detected\n",
      "112\n",
      "+++++ 108.858911\n",
      "Known face detected\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "# cap.set(3,width)\n",
    "# cap.set(4,height)\n",
    "cap.set(3,640)\n",
    "cap.set(4,480)\n",
    "background=cv2.imread(r\"C:\\Users\\ayush\\OneDrive\\Pictures\\background.png\")\n",
    "modeType=0\n",
    "\n",
    "attendence=dict.fromkeys(studentId,0)\n",
    "\n",
    "counter=0\n",
    "sid=-1\n",
    "\n",
    "bucket=storage.bucket()\n",
    "imgStudent=[]\n",
    "studentInfo=[]\n",
    "\n",
    "while True:\n",
    "    success,img=cap.read()\n",
    "    \n",
    "    imgS=cv2.resize(img,(0,0),None,0.25,0.25)\n",
    "    imgS=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    background[162:162 + 480, 55:55 + 640] = img\n",
    "    background[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "    \n",
    "#     background[162 + 480, 55 + 640] = img\n",
    "#     background[44 + 633, 808 + 414] = imgModeList[modeType]\n",
    "#     ValueError: could not broadcast input array from shape (480,640,3) into shape (640,3)\n",
    "    \n",
    "    faceCurFrame=face_recognition.face_locations(imgS)\n",
    "    encodeCurFrame=face_recognition.face_encodings(imgS,faceCurFrame)\n",
    "    \n",
    "    if faceCurFrame:\n",
    "        for encodeFace,faceLoc in zip(encodeCurFrame,faceCurFrame):\n",
    "            \n",
    "            matches=face_recognition.compare_faces(encodeListKnown,encodeFace,tolerance=0.5)\n",
    "           # distance = sqrt(sum((a - b)^2)) it calculate the distance of two face encoding\n",
    "           # the shortest distance of set of faces implies that the faces are similar\n",
    "           # face_recognition.face_distance(listOfKnownFaces,unkownfaces)\n",
    "           # it gives a booblean list whether the unkown faces mached with any of the faces\n",
    "            faceDis=face_recognition.face_distance(encodeListKnown,encodeFace)\n",
    "           # if the face_recognition.compare_faces() returns true for a face then we\n",
    "           # calculate the distance between them is distance is also min compare to all other faces\n",
    "           # then it is confirm that both the two faces are equal\n",
    "            #print(\"matches\",matches)\n",
    "            #print(\"faceDis\",faceDis)\n",
    "            matchIndex=np.argmin(faceDis)\n",
    "           #y1,x2,y2,x1=y1*4,x2*4,y2*4,x1*4\n",
    "    #         bbox=x1+55,y1+162,x2-x1,y2-y1\n",
    "            #cv2.rectangle(background,(x1,y1),(x2+x1,y2+y1),(255,0,255),15)\n",
    "            # rt= rectangle thickness\n",
    "            #cvzone.cornerRect(background,bbox,rt=0)\n",
    "            #print(faceLoc)\n",
    "            if matches[matchIndex]:\n",
    "                y1,x2,y2,x1=faceLoc\n",
    "                bbox=x1+55,y1+162,x2-x1,y2-y1\n",
    "                print(\"Known face detected\")\n",
    "                sid=studentId[matchIndex]\n",
    "                print(sid)\n",
    "                # draw rectangle around the face with rectangle thickness=0 it only display 4 _ around the corner\n",
    "                cvzone.cornerRect(background,bbox,rt=0)\n",
    "                if counter==0:\n",
    "                    counter=1\n",
    "                    modeType=1\n",
    "                    # there is a time lag when the face is detected \n",
    "                    # so just for good ui we give loading on the screen with time lag of 1 ms\n",
    "                    cvzone.putTextRect(background,\"Loading\",(275,400))\n",
    "                    cv2.imshow(\"img\",background)\n",
    "                    cv2.waitKey(1)\n",
    "            if counter!=0:\n",
    "                if counter==1:\n",
    "                    studentInfo=db.reference(f'Students/{sid}').get()\n",
    "                    #print(studentInfo)\n",
    "\n",
    "                    # get the img from the storage\n",
    "                    blob=bucket.blob(f'{sid}')\n",
    "\n",
    "                    datetimeObject=datetime.datetime.strptime(studentInfo['last_attendence_time'],\n",
    "                                                   \"%Y-%m-%d %H:%M:%S\")\n",
    "                    secondsElapsed=float((datetime.datetime.now()-datetimeObject).total_seconds())\n",
    "                    print(\"+++++\",secondsElapsed)\n",
    "                    if secondsElapsed>30:\n",
    "                        # standard meathod of conversion\n",
    "                        array=np.frombuffer(blob.download_as_string(),np.uint8)\n",
    "                        imgStudent=cv2.imdecode(array,cv2.COLOR_BGRA2BGR)\n",
    "                        ref=db.reference(f'Students/{sid}')\n",
    "                        studentInfo[\"total_attendence\"]+=1\n",
    "                        ref.child(\"total_attendence\").set(studentInfo[\"total_attendence\"])\n",
    "                        curTime=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        ref.child(\"last_attendence_time\").set(curTime)\n",
    "                    else:\n",
    "                        modeType=3\n",
    "                        background[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "                        imgStudent=[]\n",
    "                        studentInfo=[]\n",
    "                        counter=0\n",
    "                if modeType!=3:    \n",
    "                    if 10<counter<20:\n",
    "                        modeType=2\n",
    "                        background[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "\n",
    "                    if counter<=10:\n",
    "                        modeType=1\n",
    "                        background[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "                        cv2.putText(background,str(studentInfo['total_attendence']),(861,125),cv2.FONT_HERSHEY_PLAIN,\n",
    "                                   2,(255,255,255),1)\n",
    "                        cv2.putText(background,str(studentInfo['major']),(1003,550),cv2.FONT_HERSHEY_PLAIN,\n",
    "                                   0.6,(255,255,255),1)\n",
    "                        cv2.putText(background,str(sid),(1003,493),cv2.FONT_HERSHEY_PLAIN,\n",
    "                                   0.6,(255,255,255),1)\n",
    "                        cv2.putText(background,str(studentInfo['starting_year']),(1120,625),cv2.FONT_HERSHEY_PLAIN,\n",
    "                                    1,(100,100,100),1)\n",
    "                        cv2.putText(background,str(studentInfo['year']+\" Year\"),(1007,625),cv2.FONT_HERSHEY_PLAIN,\n",
    "                                   0.9,(100,100,100),1)\n",
    "                        cv2.putText(background,str(studentInfo['sex']),(910,625),cv2.FONT_HERSHEY_PLAIN,\n",
    "                                   1,(100,100,100),1)\n",
    "\n",
    "                        (w,h),_=cv2.getTextSize(studentInfo['name'],cv2.FONT_HERSHEY_PLAIN,1,1)\n",
    "                        offset=int((414-w)/2)\n",
    "                        cv2.putText(background,str(studentInfo['name']),(808+offset,445),cv2.FONT_HERSHEY_PLAIN,\n",
    "                                   1,(50,50,50),1)\n",
    "\n",
    "                        background[175:175+216,909:909+216]=imgStudent\n",
    "                counter+=1\n",
    "                if counter>=20:\n",
    "                    modeType=0\n",
    "                    counter=0\n",
    "                    imgStudent=[]\n",
    "                    studentInfo=[]\n",
    "                    background[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "    else:\n",
    "        # if no face is detected then we \n",
    "        modeType=0\n",
    "        modeType=0\n",
    "        counter=0\n",
    "        imgStudent=[]\n",
    "        studentInfo=[]\n",
    "        background[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "                \n",
    "    \n",
    "    cv2.imshow(\"img\",background)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF==ord('a'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c8e37",
   "metadata": {},
   "source": [
    "##### face_recognition.api.compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.6)[source]\n",
    "##### Compare a list of face encodings against a candidate encoding to see if they match.\n",
    "\n",
    "##### Parameters:\t\n",
    "##### known_face_encodings – A list of known face encodings\n",
    "##### face_encoding_to_check – A single face encoding to compare against the list\n",
    "##### tolerance – How much distance between faces to consider it a match. Lower is more strict. 0.6 is typical best performance.\n",
    "* https://face-recognition.readthedocs.io/en/latest/face_recognition.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a3d29",
   "metadata": {},
   "source": [
    "* face_recognition.api.face_encodings(face_image, known_face_locations=None, num_jitters=1, model='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c82dac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-11 00:54:34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__radd__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rsub__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " 'astimezone',\n",
       " 'combine',\n",
       " 'ctime',\n",
       " 'date',\n",
       " 'day',\n",
       " 'dst',\n",
       " 'fold',\n",
       " 'fromisocalendar',\n",
       " 'fromisoformat',\n",
       " 'fromordinal',\n",
       " 'fromtimestamp',\n",
       " 'hour',\n",
       " 'isocalendar',\n",
       " 'isoformat',\n",
       " 'isoweekday',\n",
       " 'max',\n",
       " 'microsecond',\n",
       " 'min',\n",
       " 'minute',\n",
       " 'month',\n",
       " 'now',\n",
       " 'replace',\n",
       " 'resolution',\n",
       " 'second',\n",
       " 'strftime',\n",
       " 'strptime',\n",
       " 'time',\n",
       " 'timestamp',\n",
       " 'timetuple',\n",
       " 'timetz',\n",
       " 'today',\n",
       " 'toordinal',\n",
       " 'tzinfo',\n",
       " 'tzname',\n",
       " 'utcfromtimestamp',\n",
       " 'utcnow',\n",
       " 'utcoffset',\n",
       " 'utctimetuple',\n",
       " 'weekday',\n",
       " 'year']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(datetime.datetime.strptime('2023-03-11 00:54:34',  \"%Y-%m-%d %H:%M:%S\"))\n",
    "dir(datetime.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614c38b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAXYEAR',\n",
       " 'MINYEAR',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'date',\n",
       " 'datetime',\n",
       " 'datetime_CAPI',\n",
       " 'sys',\n",
       " 'time',\n",
       " 'timedelta',\n",
       " 'timezone',\n",
       " 'tzinfo']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f733e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-03-13 05:16:00'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd36a9",
   "metadata": {},
   "source": [
    "###### The first line of code, background[162:162 + 480, 55:55 + 640] = img, assigns the values of img to a rectangular region of background. Specifically, it selects a sub-region of background starting at row 162 and column 55, with a height of 480 pixels and a width of 640 pixels, and replaces the values in that sub-region with the corresponding values from img.\n",
    "\n",
    "###### The second line of code, background[162 + 480, 55 + 640] = img, assigns the values of img to a single pixel in background. Specifically, it selects the pixel at row 642 (162+480) and column 695 (55+640), and replaces the value of that pixel with the corresponding value from img.\n",
    "\n",
    "###### In summary, the first line of code assigns the values of img to a rectangular sub-region of background, while the second line of code assigns the values of img to a single pixel in background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77042f6",
   "metadata": {},
   "source": [
    "* The face_recognition.face_encodings() function from the face_recognition library is used to generate a numerical encoding (a feature vector) for a face in an image. This encoding can be used for tasks such as face recognition and face comparison.\n",
    "* When calling the face_encodings() function, you can optionally pass in the face locations as an argument. The face location is a tuple of (top, right, bottom, left) coordinates that define the bounding box of the face in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f938774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import face_recognition\n",
    "\n",
    "# image = face_recognition.load_image_file(\"my_image.jpg\")\n",
    "# face_locations = face_recognition.face_locations(image)\n",
    "# face_encodings = face_recognition.face_encodings(image, face_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8313b1",
   "metadata": {},
   "source": [
    "* In this example, face_locations is a list of tuples that contains the bounding box coordinates of all the faces in the image. By passing face_locations as an argument to face_encodings(), the function will only generate encodings for the faces in the specified locations.\n",
    "\n",
    "* On the other hand, if you don't pass the face locations to the face_encodings() function, it will try to find all the faces in the image automatically using a built-in face detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4cc0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import face_recognition\n",
    "\n",
    "# image = face_recognition.load_image_file(\"my_image.jpg\")\n",
    "# face_encodings = face_recognition.face_encodings(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0621fc3",
   "metadata": {},
   "source": [
    "* In this case, the face_encodings() function will try to find all the faces in the image and generate encodings for each face it finds.\n",
    "\n",
    "* So the main difference between calling face_encodings() with face locations and without face locations is that in the former, you are explicitly telling the function which faces to generate encodings for, whereas in the latter, the function will try to find all the faces in the image automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d998929",
   "metadata": {},
   "source": [
    "# mode = 0 active\n",
    "# mode =1 details\n",
    "# mode=2 thank you\n",
    "# mode=3 already marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77514e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04840c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1280e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15200822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5b0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
